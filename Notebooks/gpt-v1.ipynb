{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31194ee3-7eb3-4862-80d5-136d9d5af25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "import os.path\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 3000\n",
    "# learning_rate = 3e-3, 3e-4, 1e-3, 1e-4\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "dropout = 0.2 # dropout random neurons in the network to avoid overfitting\n",
    "n_embd = 384\n",
    "# embedding_vector = [0.1, 0.2, 0.8, 1.1] # ^^would be 384 elements long in the vector\n",
    "n_head = 16\n",
    "n_layer = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0aefb-2478-4404-8d2d-0077c7d67dea",
   "metadata": {},
   "source": [
    "How many decoder layers do we have?<br>\n",
    "Could have Sequential neural network with 4 decoder layers (or decoder blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b114e125-206f-419a-bd6a-4fb444a7129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "with open('openwebtext/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee212d7-b782-45b9-bf21-c98278401245",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14070488-8b38-4eba-ae27-edd3cec52f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use memory mapping to get the text that we need\n",
    "def get_random_chunk(split):\n",
    "    filename = \"openwebtext/train_split.txt\" if split == 'train' else \"openwebtext/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # determine file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random positiona and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    # initialize ix - rand integer between 0 and lenght of text - block_size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    # Stack them in batches\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2ebdd8-f407-4831-809b-8d09d0dc3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # put the model into training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6029ea-8177-4a44-8504-00c6051f82fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model parameters...\n",
      "loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size(batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,hs)\n",
    "        q = self.query(x) #(B,T,hs)\n",
    "        # compute attention scores(affinities)\n",
    "        # transpose puts the matrices into the correct shape that they can be multiplied correctly\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) #(B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation o fthe values\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# masked fill: [1, 0.6, 0.4]\n",
    "# mask out everything except for the first time-step\n",
    "# on the next time-step then expose the next piece\n",
    "# [1, 0, 0] -> [1, 0.6, 0] -> [1, 0.6, 0.4]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # an extra learnable parameters\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Last dimension (B x T x F(Features)) -> (B, T, [h1, h1, h1, h2, h2, h2, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    # Each of these decoder blocks is a block from the diagram model\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # n_embd -> we want the tokens to be in a really large vector\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        # Broadcasting semantics - rules about how you perform arithmetic operations on a tensor\n",
    "        x = tok_emb + pos_emb # (B, T, C) shape\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            # there are not just one or two normalizations - there are many of them\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index,index_next), dim=1) #(B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "if os.path.isfile('model-01.pkl'):\n",
    "    print('loading model parameters...')\n",
    "    with open('model-01.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print('loaded successfully')\n",
    "    m = model.to(device)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cb4f3-3d1e-4dca-89b0-67b384b6fb9e",
   "metadata": {},
   "source": [
    "# activation functions\n",
    "nn.Linear() in a row\n",
    "Would apply one transformation that would multiply themselves together\n",
    "So what can we do that is more than linear transformations\n",
    "3 activation functions will help with the computational part of this\n",
    "functional.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15d4c3d-52c3-48f0-bac2-079868532ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 1.291, val loss: 1.169\n",
      "step: 100, train loss: 1.194, val loss: 1.240\n",
      "step: 200, train loss: 1.236, val loss: 1.218\n",
      "step: 300, train loss: 1.184, val loss: 1.221\n",
      "step: 400, train loss: 1.242, val loss: 1.204\n",
      "step: 500, train loss: 1.215, val loss: 1.216\n",
      "step: 600, train loss: 1.210, val loss: 1.257\n",
      "step: 700, train loss: 1.215, val loss: 1.206\n",
      "step: 800, train loss: 1.241, val loss: 1.241\n",
      "step: 900, train loss: 1.220, val loss: 1.206\n",
      "step: 1000, train loss: 1.201, val loss: 1.253\n",
      "step: 1100, train loss: 1.237, val loss: 1.174\n",
      "step: 1200, train loss: 1.199, val loss: 1.181\n",
      "step: 1300, train loss: 1.253, val loss: 1.244\n",
      "step: 1400, train loss: 1.255, val loss: 1.203\n",
      "step: 1500, train loss: 1.255, val loss: 1.290\n",
      "step: 1600, train loss: 1.220, val loss: 1.215\n",
      "step: 1700, train loss: 1.202, val loss: 1.212\n",
      "step: 1800, train loss: 1.174, val loss: 1.191\n",
      "step: 1900, train loss: 1.187, val loss: 1.207\n",
      "step: 2000, train loss: 1.214, val loss: 1.195\n",
      "step: 2100, train loss: 1.161, val loss: 1.222\n",
      "step: 2200, train loss: 1.181, val loss: 1.204\n",
      "step: 2300, train loss: 1.200, val loss: 1.201\n",
      "step: 2400, train loss: 1.162, val loss: 1.195\n",
      "step: 2500, train loss: 1.188, val loss: 1.180\n",
      "step: 2600, train loss: 1.211, val loss: 1.163\n",
      "step: 2700, train loss: 1.194, val loss: 1.193\n",
      "step: 2800, train loss: 1.214, val loss: 1.196\n",
      "step: 2900, train loss: 1.235, val loss: 1.194\n",
      "1.0519119501113892\n"
     ]
    }
   ],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\") # single quotes mean braces and brackets are the same\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ece5-18fc-4fdc-a15c-ba35d1126f9f",
   "metadata": {},
   "source": [
    "We are looking at the Standard Deviation function<br>\n",
    "sum(xi) = 0.6 from arr = [0.1,0.2,0.3]<br>\n",
    "std deviation of: [-0.38, 0.52, 2.48]<br>\n",
    "N = 3 (number of elements)<br>\n",
    "mean = average of sum / N<br>\n",
    "mean = 0.87333333333<br>\n",
    "sq_rt{1/3 * sigma(xi - 0.87)^2}<br>\n",
    "std_dev = ~1.19<br>\n",
    "When initializing parameters - if you have outliers in your data they will really stretch out the standard deviation and you will not learn anything<br>\n",
    "If you have data too close together then they all learn the same thing, you need a balanced and stable network.<br>\n",
    "@4:40 in the video look at how he is breaking apart the large file you downloaded<br>\n",
    "OpenWeb Text which OpenAI used to train its model<br>\n",
    "https://skylion007.github.io/OpenWebTextCorpus/<br>\n",
    "<br>\n",
    "As we scale up we will use torch.load and torch.save to store those parameters in the model-01 file that we create and load<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e0bd9-00b0-490d-8553-28eb5fe993d6",
   "metadata": {},
   "source": [
    "Final Notes:<br>\n",
    "It doesn't have useful output right now.  However, with another course in fine tuning we could.  But this builds on the whole architecture and gives<br>\n",
    "you a good idea of how you would build a simple chatbot.<br>\n",
    "\n",
    "- efficiency testing<br>\n",
    "- RNNs - recurrent neural networks<br>\n",
    "    - sequential learning<br>\n",
    "    - a little dumber than transformers<br>\n",
    "    - runs on CPU<br>\n",
    "- Quantization - reduce the memory usage by your parameters<br>\n",
    "    - Paper: QLoRA: Efficient Finetuning of Quantized LLMs<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45898c-d3b5-478a-aa61-b28570d14556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
